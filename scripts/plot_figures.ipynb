{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb\n",
    "\n",
    "dataset_match = {\n",
    "    \"fbirn\": \"FBIRN ICA\",\n",
    "    \"fbirn_roi\": \"FBIRN Schaefer 200 ROI\",\n",
    "    \"bsnip\": \"BSNIP ICA\",\n",
    "    \"cobre\": \"COBRE ICA\",\n",
    "\n",
    "    \"abide\": \"ABIDE ICA 569 subjects\",\n",
    "    \"abide_869\": \"ABIDE ICA\",\n",
    "    \"abide_roi\": \"ABIDE Schaefer 200 ROI\",\n",
    "\n",
    "    \"oasis\": \"OASIS ICA\",\n",
    "    \"adni\": \"ADNI ICA\",\n",
    "\n",
    "    \"hcp\": \"HCP ICA\",\n",
    "    \"hcp_roi\": \"HCP Schaefer 200 ROI Extended\",\n",
    "    \"hcp_roi_752\": \"HCP Schaefer 200 ROI\",\n",
    "    \"hcp_schaefer\": \"HCP Schaefer 200 ROI w/o denoising\",\n",
    "    \"hcp_non_mni_2\": \"HCP Deskian/Killiany ROI in Orig space\",\n",
    "    \"hcp_mni_3\": \"HCP Deskian/Killiany ROI\",\n",
    "\n",
    "    \"ukb\": \"UK Biobank (Sex) ICA\",\n",
    "    \"ukb_age_bins\": \"UK Biobank (Age-Sex) ICA\",\n",
    "\n",
    "    \"hcp_time\": \"HCP ICA Time\",\n",
    "\n",
    "    \"fbirn_cobre\": \"FBIRN X COBRE\",\n",
    "    \"fbirn_bsnip\": \"FBIRN X BSNIP\",\n",
    "    \"cobre_fbirn\": \"COBRE X FBIRN\",\n",
    "    \"cobre_bsnip\": \"COBRE X BSNIP\",\n",
    "    \"bsnip_fbirn\": \"BSNIP X FBIRN\",\n",
    "    \"bsnip_cobre\": \"BSNIP X COBRE\",\n",
    "\n",
    "    \"oasis_adni\": \"OASIS X ADNI\",\n",
    "    \"adni_oasis\": \"ADNI X OASIS\",\n",
    "}\n",
    "\n",
    "model_match = {\n",
    "    \"mlp\": \"Old Mean MLP\",\n",
    "    \"rearranged_mlp\": \"Mean MLP\",\n",
    "    \"attn_mlp\": \"Attention MLP\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"mean_lstm\": \"Mean LSTM\",\n",
    "    \"transformer\": \"Transformer w/o PE\",\n",
    "    \"mean_transformer\": \"Mean Transformer w/o PE\",\n",
    "    \"pe_transformer\": \"Transformer\",\n",
    "    \"mean_pe_transformer\": \"Mean Transformer\",\n",
    "    \"milc\": \"MILC\",\n",
    "    \"dice\": \"DICE\",\n",
    "    \"bnt\": \"BNT\",\n",
    "    \"fbnetgen\": \"FBNetGen\",\n",
    "    \"brainnetcnn\": \"BrainNetCNN\",\n",
    "    \"lr\": \"LR\",\n",
    "}\n",
    "\n",
    "def fix_run(run, dataset):\n",
    "    for _, row in run.history().iterrows():\n",
    "        if f\"{dataset}score\" in row and f\"{dataset}accuracy\" in row and \"training_time\" in row and f\"{dataset}average_time\" in row:\n",
    "            if row[f\"{dataset}score\"] is not None and row[f\"{dataset}accuracy\"] is not None and row[\"training_time\"] is not None and row[f\"{dataset}average_time\"] is not None:\n",
    "                if not isnan(row[f\"{dataset}score\"]) and not isnan(row[f\"{dataset}accuracy\"]) and not isnan(row[\"training_time\"]) and not isnan(row[f\"{dataset}average_time\"]):\n",
    "                    auc = row[f\"{dataset}score\"]\n",
    "                    acc = row[f\"{dataset}accuracy\"]\n",
    "                    train_t = row[\"training_time\"]\n",
    "                    inf_t= row[f\"{dataset}average_time\"]\n",
    "                    break\n",
    "    \n",
    "    run.summary[f\"{dataset}score\"] = auc\n",
    "    run.summary[f\"{dataset}accuracy\"] = acc\n",
    "    run.summary[\"training_time\"] = train_t\n",
    "    run.summary[f\"{dataset}average_time\"] = inf_t\n",
    "\n",
    "    run.summary.update()\n",
    "\n",
    "    return auc, acc, train_t, inf_t\n",
    "\n",
    "def load_run(proj_name, dataset):\n",
    "    if dataset is not None:\n",
    "        dataset += \"_\"\n",
    "    else:\n",
    "        dataset = \"test_\"\n",
    "\n",
    "    api = wandb.Api(timeout=19)\n",
    "    runs = api.runs(f\"pavalipopov/{proj_name}\")\n",
    "\n",
    "    summary_list = []\n",
    "    runs_list = []\n",
    "    config_list = []\n",
    "    for i, run in enumerate(runs):\n",
    "        summary_list.append(run.summary._json_dict)\n",
    "        config_list.append(run.config)\n",
    "        runs_list.append(run)\n",
    "\n",
    "\n",
    "    AUC_score = []\n",
    "    accuracy = []\n",
    "    train_time = []\n",
    "    inference_time = []\n",
    "    params = []\n",
    "    shuffling = []\n",
    "\n",
    "    for i, summary in enumerate(summary_list):\n",
    "        everything_is_cool = True\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}score\" in summary and summary[f\"{dataset}score\"] is not None\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}accuracy\" in summary and summary[f\"{dataset}accuracy\"] is not None\n",
    "        everything_is_cool = everything_is_cool and \"training_time\" in summary and summary[\"training_time\"] is not None\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}average_time\" in summary and summary[f\"{dataset}average_time\"] is not None\n",
    "\n",
    "        if everything_is_cool:\n",
    "            AUC_score.append(summary[f\"{dataset}score\"])\n",
    "            accuracy.append(summary[f\"{dataset}accuracy\"])\n",
    "            train_time.append(summary[\"training_time\"])\n",
    "            inference_time.append(summary[f\"{dataset}average_time\"])\n",
    "        else:\n",
    "            print(f\"Run {i} summary is broken, fixing it\")\n",
    "            auc, acc, train_t, inf_t = fix_run(runs_list[i], dataset)\n",
    "            AUC_score.append(auc)\n",
    "            accuracy.append(acc)\n",
    "            train_time.append(train_t)\n",
    "            inference_time.append(inf_t)\n",
    "\n",
    "        if \"params\" in summary:\n",
    "            params.append(summary[\"params\"])\n",
    "        else:\n",
    "            params.append(None)\n",
    "\n",
    "        if \"permute\" in config_list[i][\"general\"]:\n",
    "            if config_list[i][\"general\"][\"permute\"] == False:\n",
    "                shuffling.append(\"None\")\n",
    "            else:\n",
    "                shuffling.append(config_list[i][\"general\"][\"permute\"])\n",
    "        else:\n",
    "            shuffling.append(\"None\")\n",
    "    \n",
    "    return AUC_score, accuracy, train_time, inference_time, params, shuffling\n",
    "\n",
    "def load_metrics(paths_dict, ds_dict, model_dict):\n",
    "    data_list = []\n",
    "\n",
    "    for model_name in paths_dict:\n",
    "        print(model_name)\n",
    "        for dataset_name in paths_dict[model_name]:\n",
    "            print(\"\\t \", dataset_name)\n",
    "            \n",
    "            if len(paths_dict[model_name][dataset_name]) == 2:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = paths_dict[model_name][dataset_name][1]\n",
    "            elif len(paths_dict[model_name][dataset_name]) == 1:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            print(\"\\t\\t sub_dataset:\", dataset)\n",
    "\n",
    "            AUC_score, accuracy, train_time, inference_time, params, shuffling = load_run(path, dataset)\n",
    "            \n",
    "            if dataset != \"test\":\n",
    "                dataset = dataset_name + \"_\" + dataset\n",
    "            else:\n",
    "                dataset = dataset_name\n",
    "            length = len(AUC_score)\n",
    "\n",
    "            data_list.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\": [model_dict[model_name]]*length,\n",
    "                        \"Dataset\": [ds_dict[dataset]]*length,\n",
    "                        \"AUROC\": AUC_score,\n",
    "                        \"Accuracy\": accuracy,\n",
    "                        \"Train time\": train_time,\n",
    "                        \"Inference time\": inference_time,\n",
    "                        \"Trainable params\": params,\n",
    "                        \"Shuffling\": shuffling,\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    return pd.concat(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(proj_name, dataset):\n",
    "    if dataset is not None:\n",
    "        dataset += \"_\"\n",
    "    else:\n",
    "        dataset = \"test_\"\n",
    "\n",
    "    api = wandb.Api(timeout=19)\n",
    "    # Project is specified by <entity/project-name>\n",
    "\n",
    "    try:\n",
    "        runs = api.runs(f\"pavalipopov/{proj_name}\")\n",
    "        summary_list = []\n",
    "        runs_list = []\n",
    "        config_list = []\n",
    "        for i, run in enumerate(runs): \n",
    "            # print(f\"Run {i}\")\n",
    "            # .summary contains the output keys/values for metrics like accuracy.\n",
    "            #  We call ._json_dict to omit large files \n",
    "            summary_list.append(run.summary._json_dict)\n",
    "            config_list.append(run.config)\n",
    "            runs_list.append(run)\n",
    "    except:\n",
    "        print(proj_name)\n",
    "        return\n",
    "\n",
    "    global_cool = True\n",
    "\n",
    "    count = 0\n",
    "    for i, summary in enumerate(summary_list):\n",
    "        everything_is_cool = True\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}score\" in summary and summary[f\"{dataset}score\"] is not None\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}accuracy\" in summary and summary[f\"{dataset}accuracy\"] is not None\n",
    "        everything_is_cool = everything_is_cool and \"training_time\" in summary and summary[\"training_time\"] is not None\n",
    "        everything_is_cool = everything_is_cool and f\"{dataset}average_time\" in summary and summary[f\"{dataset}average_time\"] is not None\n",
    "\n",
    "        if not everything_is_cool:\n",
    "            try:\n",
    "                fix_run(runs_list[i], dataset)\n",
    "                count += 1\n",
    "            except:\n",
    "                global_cool = False\n",
    "            # print(f\"Run {i} summary is broken\")\n",
    "        else:\n",
    "            count += 1\n",
    "    \n",
    "    if not global_cool or count != 50:\n",
    "        print(proj_name)\n",
    "\n",
    "def test_runs(paths_dict):\n",
    "\n",
    "    print(\"Problematic runs:\")\n",
    "\n",
    "    for model_name in paths_dict:\n",
    "        # print(model_name)\n",
    "        for dataset_name in paths_dict[model_name]:\n",
    "            # print(\"\\t \", dataset_name)\n",
    "            \n",
    "            if len(paths_dict[model_name][dataset_name]) == 2:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = paths_dict[model_name][dataset_name][1]\n",
    "            elif len(paths_dict[model_name][dataset_name]) == 1:\n",
    "                path = paths_dict[model_name][dataset_name][0]\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # print(\"\\t\\t sub_dataset:\", dataset)\n",
    "\n",
    "            test_run(path, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"fbirn\", \n",
    "    \"fbirn_roi\",\n",
    "    \"bsnip\",\n",
    "    \"cobre\",\n",
    "    \"abide_869\",\n",
    "    \"abide_roi\",\n",
    "    \"oasis\",\n",
    "    \"adni\",\n",
    "    \"hcp\",\n",
    "    \"hcp_roi_752\",\n",
    "    \"ukb\",\n",
    "    \"ukb_age_bins\",\n",
    "]\n",
    "dataset_suffix = \"_th\"\n",
    "models = [\n",
    "    # \"mlp\",\n",
    "    \"lstm\",\n",
    "    # \"transformer\",\n",
    "    \"milc\",\n",
    "    \"dice\",\n",
    "    \"bnt\",\n",
    "    \"fbnetgen\",\n",
    "    \"brainnetcnn\",\n",
    "    \"lr\",\n",
    "]\n",
    "model_suffix = \"_defHP\"\n",
    "\n",
    "prefix = \"rerun_all\"\n",
    "suffix = \"\"\n",
    "\n",
    "general_projects = {}\n",
    "for model in models:\n",
    "    general_projects[model] = {}\n",
    "\n",
    "    project_model = model\n",
    "    if model_suffix is not None:\n",
    "        project_model += model_suffix\n",
    "\n",
    "    for dataset in datasets:\n",
    "        project_dataset = dataset\n",
    "        if dataset in [\"ukb\", \"ukb_age_bins\"]:\n",
    "            project_dataset += dataset_suffix\n",
    "\n",
    "        general_projects[model][dataset] = (f\"{prefix}-exp-{project_model}-{project_dataset}{suffix}\", )\n",
    "        if model == \"lr\" and dataset in [\"fbirn\", \"cobre\", \"bsnip\", \"oasis\", \"adni\"]:\n",
    "            general_projects[model][dataset] = (f\"cross_ds-exp-{project_model}-{project_dataset}{suffix}\", )\n",
    "\n",
    "models = [\n",
    "    \"rearranged_mlp\",\n",
    "    \"mean_lstm\",\n",
    "    # \"mean_transformer\",\n",
    "    \"pe_transformer\",\n",
    "    \"mean_pe_transformer\",\n",
    "]\n",
    "prefix = \"rerun_all2\"\n",
    "for model in models:\n",
    "    general_projects[model] = {}\n",
    "\n",
    "    project_model = model\n",
    "    if model_suffix is not None:\n",
    "        project_model += model_suffix\n",
    "\n",
    "    for dataset in datasets:\n",
    "        project_dataset = dataset\n",
    "        if dataset in [\"ukb\", \"ukb_age_bins\"]:\n",
    "            project_dataset += dataset_suffix\n",
    "\n",
    "        general_projects[model][dataset] = (f\"{prefix}-exp-{project_model}-{project_dataset}{suffix}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = load_metrics(general_projects, dataset_match, model_match)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = comparison_data.query(\n",
    "        \"Shuffling == 'None'\")\n",
    "\n",
    "models = [\n",
    "    \"Mean MLP\",\n",
    "    'LSTM',\n",
    "    'Transformer',\n",
    "    # 'Mean LSTM',\n",
    "    # 'Mean Transformer',\n",
    "    # 'PE Transformer',\n",
    "    # 'Mean PE Transformer',\n",
    "    \"MILC\", \n",
    "    \"DICE\", \n",
    "    \"BNT\", \n",
    "    \"FBNetGen\", \n",
    "    \"BrainNetCNN\", \n",
    "    \"LR\"\n",
    "]\n",
    "datasets = [\n",
    "    \"FBIRN ICA\",\n",
    "    \"BSNIP ICA\",\n",
    "    \"COBRE ICA\",\n",
    "    \"ABIDE ICA\",\n",
    "    \"OASIS ICA\",\n",
    "    \"ADNI ICA\",\n",
    "    \"HCP ICA\",\n",
    "    \"UK Biobank (Sex) ICA\",\n",
    "    \"UK Biobank (Age-Sex) ICA\",\n",
    "\n",
    "    \"FBIRN Schaefer 200 ROI\",\n",
    "    \"ABIDE Schaefer 200 ROI\",\n",
    "    \"HCP Schaefer 200 ROI\",\n",
    "\n",
    "    \"HCP ICA Time\",\n",
    "]\n",
    "\n",
    "rename_dict = {\n",
    "    \"FBIRN ICA\": \"FBIRN\",\n",
    "    \"BSNIP ICA\": \"BSNIP\",\n",
    "    \"COBRE ICA\": \"COBRE\",\n",
    "    \"ABIDE ICA\": \"ABIDE\",\n",
    "    \"OASIS ICA\": \"OASIS\",\n",
    "    \"ADNI ICA\": \"ADNI\",\n",
    "    \"HCP ICA\": \"HCP\",\n",
    "    \"UK Biobank (Sex) ICA\": \"UKB-S\",\n",
    "    \"UK Biobank (Age-Sex) ICA\": \"UKB-SA\",\n",
    "\n",
    "    \"FBIRN Schaefer 200 ROI\": \"FBIRN\",\n",
    "    \"ABIDE Schaefer 200 ROI\": \"ABIDE\",\n",
    "    \"HCP Schaefer 200 ROI\": \"HCP\",\n",
    "\n",
    "    \"HCP ICA Time\": \"HCP\",\n",
    "}\n",
    "\n",
    "hue_order = models\n",
    "\n",
    "ica_datasets = datasets[:7]\n",
    "data_1 = plot_data.query(\"Dataset in @ica_datasets\")\n",
    "data_1[\"Dataset\"] = data_1[\"Dataset\"].replace(rename_dict)\n",
    "ica_datasets = [rename_dict[dataset] for dataset in ica_datasets]\n",
    "\n",
    "ukb_s_datasets = datasets[7:8]\n",
    "data_s_ukb = plot_data.query(\"Dataset in @ukb_s_datasets\")\n",
    "data_s_ukb[\"Dataset\"] = data_s_ukb[\"Dataset\"].replace(rename_dict)\n",
    "ukb_s_datasets = [rename_dict[dataset] for dataset in ukb_s_datasets]\n",
    "\n",
    "ukb_sa_datasets = datasets[8:9]\n",
    "data_sa_ukb = plot_data.query(\"Dataset in @ukb_sa_datasets\")\n",
    "data_sa_ukb[\"Dataset\"] = data_sa_ukb[\"Dataset\"].replace(rename_dict)\n",
    "ukb_sa_datasets = [rename_dict[dataset] for dataset in ukb_sa_datasets]\n",
    "\n",
    "roi_datasets = datasets[9:12]\n",
    "data_2 = plot_data.query(\"Dataset in @roi_datasets\")\n",
    "data_2[\"Dataset\"] = data_2[\"Dataset\"].replace(rename_dict)\n",
    "roi_datasets = [rename_dict[dataset] for dataset in roi_datasets]\n",
    "\n",
    "time_datasets = datasets[12:]\n",
    "data_3 = plot_data.query(\"Dataset in @time_datasets\")\n",
    "data_3[\"Dataset\"] = data_3[\"Dataset\"].replace(rename_dict)\n",
    "time_datasets = [rename_dict[dataset] for dataset in time_datasets]\n",
    "\n",
    "palette = { item: plt.cm.tab20(i) for i, item in enumerate(models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\", \n",
    "    font_scale = 1.5,\n",
    ")\n",
    "\n",
    "# Create a figure and a grid of subplots (2 rows, 1 column)\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 1], hspace=0.3)\n",
    "\n",
    "# Create subplots for each subfigure\n",
    "subfig11 = plt.subplot(gs[0])\n",
    "\n",
    "# Divide the lower row into two subplots with different widths\n",
    "# subgs2 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[1], width_ratios=[1, 1, 3, 2])\n",
    "subgs2 = gridspec.GridSpecFromSubplotSpec(1, 4, subplot_spec=gs[1], width_ratios=[2, 2, 6, 2])\n",
    "subfig12 = plt.subplot(subgs2[0])\n",
    "subfig13 = plt.subplot(subgs2[1])\n",
    "subfig2 = plt.subplot(subgs2[2])\n",
    "subfig3 = plt.subplot(subgs2[3])\n",
    "\n",
    "### add ghost figure for UKB results title\n",
    "ghostsubgs2 = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=gs[1], width_ratios=[4, 6, 2])\n",
    "ghost_ax = fig.add_subplot(ghostsubgs2[0])\n",
    "ghost_ax.axis('off')\n",
    "ghost_ax.set_title('ICA - Neuromark')\n",
    "###### plot data\n",
    "\n",
    "## row 1\n",
    "boxplot_1 = sns.boxplot(\n",
    "    x=\"Dataset\", \n",
    "    order=ica_datasets,\n",
    "    y=\"AUROC\",\n",
    "    hue=\"Model\",\n",
    "    hue_order=hue_order,\n",
    "    data=data_1,\n",
    "    palette=palette,\n",
    "    showfliers = False,\n",
    "    ax=subfig11,\n",
    ")\n",
    "subfig11.legend_.remove()\n",
    "subfig11.axhline(y=0.5, linestyle='dashed')\n",
    "subfig11.set_xlabel(None)\n",
    "subfig11.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "subfig11.set_ylim(0.45, 1.05)\n",
    "subfig11.set_ylabel(\"ROC AUC\")\n",
    "subfig11.set_title('ICA - Neuromark')\n",
    "\n",
    "## row 2\n",
    "boxplot_2 = sns.boxplot(\n",
    "    x=\"Dataset\", \n",
    "    order=ukb_s_datasets,\n",
    "    y=\"AUROC\",\n",
    "    hue=\"Model\",\n",
    "    hue_order=hue_order,\n",
    "    data=data_s_ukb,\n",
    "    palette=palette,\n",
    "    showfliers = False,\n",
    "    ax=subfig12,\n",
    ")\n",
    "subfig12.legend_.remove()\n",
    "subfig12.axhline(y=0.5, linestyle='dashed')\n",
    "subfig12.set_xlabel(None)\n",
    "# subfig11.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "subfig12.set_ylim(0.935, 0.975)\n",
    "subfig12.set_ylabel(\"ROC AUC\")\n",
    "\n",
    "\n",
    "boxplot_3 = sns.boxplot(\n",
    "    x=\"Dataset\", \n",
    "    order=ukb_sa_datasets,\n",
    "    y=\"AUROC\",\n",
    "    hue=\"Model\",\n",
    "    hue_order=hue_order,\n",
    "    data=data_sa_ukb,\n",
    "    palette=palette,\n",
    "    showfliers = False,\n",
    "    ax=subfig13,\n",
    ")\n",
    "subfig13.legend_.remove()\n",
    "subfig13.axhline(y=0.5, linestyle='dashed')\n",
    "subfig13.set_xlabel(None)\n",
    "# subfig13.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "subfig13.set_ylim(0.745, 0.855)\n",
    "subfig13.set_ylabel(None)\n",
    "\n",
    "boxplot_4 = sns.boxplot(\n",
    "    x=\"Dataset\", \n",
    "    order=roi_datasets,\n",
    "    y=\"AUROC\",\n",
    "    hue=\"Model\",\n",
    "    hue_order=hue_order,\n",
    "    data=data_2,\n",
    "    palette=palette,\n",
    "    showfliers = False,\n",
    "    ax=subfig2,\n",
    ")\n",
    "subfig2.set_title('ROI - Schaefer 200 ROIs atlas')\n",
    "subfig2.legend_.remove()\n",
    "subfig2.axhline(y=0.5, linestyle='dashed')\n",
    "subfig2.set_xlabel(None)\n",
    "subfig2.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "subfig2.set_ylim(0.45, 1.05)\n",
    "subfig2.set_ylabel(None)\n",
    "\n",
    "###### Add hatches\n",
    "\n",
    "for i in range(10, 18):\n",
    "    boxplot_1.patches[i].set_hatch(\"///\")\n",
    "for i, box in enumerate(boxplot_1.patches):\n",
    "    if i > 18:\n",
    "        if i % 9 == 5 or i % 9 == 6 or i % 9 == 7 or i % 9 == 8:\n",
    "            box.set_hatch(\"///\")\n",
    "\n",
    "for i in range(10, 18):\n",
    "    boxplot_2.patches[i].set_hatch(\"///\")\n",
    "for i, box in enumerate(boxplot_2.patches):\n",
    "    if i > 18:\n",
    "        if i % 9 == 5 or i % 9 == 6 or i % 9 == 7 or i % 9 == 8:\n",
    "            box.set_hatch(\"///\")\n",
    "\n",
    "for i in range(10, 18):\n",
    "    boxplot_3.patches[i].set_hatch(\"///\")\n",
    "for i, box in enumerate(boxplot_3.patches):\n",
    "    if i > 18:\n",
    "        if i % 9 == 5 or i % 9 == 6 or i % 9 == 7 or i % 9 == 8:\n",
    "            box.set_hatch(\"///\")\n",
    "\n",
    "for i in range(10, 18):\n",
    "    boxplot_4.patches[i].set_hatch(\"///\")\n",
    "for i, box in enumerate(boxplot_4.patches):\n",
    "    if i > 18:\n",
    "        if i % 9 == 5 or i % 9 == 6 or i % 9 == 7 or i % 9 == 8:\n",
    "            box.set_hatch(\"///\")\n",
    "\n",
    "# Create a legend in subfigure 3 for the boxplots\n",
    "subfig3.axis('off')  # Turn off axis for legend subplot\n",
    "legend_handles, legend_labels = subfig11.get_legend_handles_labels()\n",
    "\n",
    "ts_models = [\n",
    "    \"Mean MLP\", \n",
    "    'LSTM',\n",
    "    'Transformer',\n",
    "    \"MILC\", \n",
    "    \"DICE\", \n",
    "]\n",
    "fnc_models = [\n",
    "    \"BNT\", \n",
    "    \"FBNetGen\", \n",
    "    \"BrainNetCNN\", \n",
    "    \"LR\"\n",
    "]\n",
    "\n",
    "ts_handles = [handle for label, handle in zip(legend_labels, legend_handles) if label in ts_models]\n",
    "ts_labels = [label for label in legend_labels if label in ts_models]\n",
    "fnc_handles = [handle for label, handle in zip(legend_labels, legend_handles) if label in fnc_models]\n",
    "fnc_labels = [label for label in legend_labels if label in fnc_models]\n",
    "\n",
    "# Create legends for each category and place them in subfig4\n",
    "legend_ts = subfig3.legend(handles=ts_handles, labels=ts_labels, title='Time Series Models', loc='upper left')\n",
    "\n",
    "# # Calculate the position for legend_fnc to the right of legend_ts\n",
    "legend_ts_bbox = legend_ts.get_bbox_to_anchor().transformed(subfig3.transAxes.inverted())\n",
    "legend_fnc = subfig3.legend(handles=fnc_handles, labels=fnc_labels, title='FNC Models', loc='upper left', bbox_to_anchor=(legend_ts_bbox.x0, legend_ts_bbox.y0+0.35))\n",
    "# legend_fnc = subfig3.legend(handles=fnc_handles, labels=fnc_labels, title='FNC Models', loc='upper left')\n",
    "\n",
    "# # Readd the TS legend box to subfig3\n",
    "subfig3.add_artist(legend_ts)\n",
    "\n",
    "# legend = subfig3.legend(handles=legend_handles,\n",
    "#                         labels=legend_labels, loc='right', title='Models')\n",
    "# subfig3.add_artist(legend)\n",
    "\n",
    "# # Adjust spacing between subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "### Add Wilcoxon tests\n",
    "\n",
    "box_pairs = [((dataset, \"Mean MLP\"), (dataset, \"DICE\")) for dataset in ica_datasets]\n",
    "box_pairs += [((dataset, \"BNT\"), (dataset, \"LR\")) for dataset in ica_datasets]\n",
    "add_stat_annotation(subfig11, data=data_1, x=\"Dataset\", y=\"AUROC\", hue=\"Model\", hue_order=hue_order,\n",
    "                    box_pairs=box_pairs,\n",
    "                    test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "                    pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "                    line_offset=0.01, line_offset_to_box=0.01, line_height=0.01, fontsize=30)\n",
    "subfig11.set_xlim(xmin=-0.5, xmax=6.5)\n",
    "subfig11.set_ylim(0.45, 1.05)\n",
    "\n",
    "box_pairs = [((dataset, \"Mean MLP\"), (dataset, \"DICE\")) for dataset in ukb_s_datasets]\n",
    "box_pairs += [((dataset, \"BNT\"), (dataset, \"LR\")) for dataset in ukb_s_datasets]\n",
    "add_stat_annotation(subfig12, data=data_s_ukb, x=\"Dataset\", y=\"AUROC\", hue=\"Model\", hue_order=hue_order,\n",
    "                    box_pairs=box_pairs,\n",
    "                    test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "                    pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "                    line_offset=0.01, line_offset_to_box=0.01, line_height=0.01, fontsize=30)\n",
    "subfig12.set_xlim(xmin=-0.5, xmax=0.5)\n",
    "subfig12.set_ylim(0.925, 0.98)\n",
    "\n",
    "box_pairs = [((dataset, \"Mean MLP\"), (dataset, \"DICE\")) for dataset in ukb_sa_datasets]\n",
    "box_pairs += [((dataset, \"BNT\"), (dataset, \"LR\")) for dataset in ukb_sa_datasets]\n",
    "add_stat_annotation(subfig13, data=data_sa_ukb, x=\"Dataset\", y=\"AUROC\", hue=\"Model\", hue_order=hue_order,\n",
    "                    box_pairs=box_pairs,\n",
    "                    test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "                    pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "                    line_offset=0.01, line_offset_to_box=0.01, line_height=0.01, fontsize=30)\n",
    "subfig13.set_xlim(xmin=-0.5, xmax=0.5)\n",
    "subfig13.set_ylim(0.745, 0.845)\n",
    "\n",
    "box_pairs = [((dataset, \"Mean MLP\"), (dataset, \"DICE\")) for dataset in roi_datasets]\n",
    "box_pairs += [((dataset, \"BNT\"), (dataset, \"LR\")) for dataset in roi_datasets]\n",
    "add_stat_annotation(subfig2, data=data_2, x=\"Dataset\", y=\"AUROC\", hue=\"Model\", hue_order=hue_order,\n",
    "                    box_pairs=box_pairs,\n",
    "                    test='Wilcoxon', text_format='star', loc='inside', verbose=1, text_offset=-4, \n",
    "                    pvalue_thresholds = [[0.05, \"*\"], [1, u'\\N{DEGREE SIGN}']],\n",
    "                    line_offset=0.01, line_offset_to_box=0.01, line_height=0.01, fontsize=30)\n",
    "subfig2.set_xlim(xmin=-0.5, xmax=2.5)\n",
    "subfig2.set_ylim(0.45, 1.05)\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig(\n",
    "    \"general_comparison.png\",\n",
    "    format=\"png\",\n",
    "    # dpi=300,\n",
    "    bbox_inches='tight',\n",
    ")\n",
    "# plt.savefig(\n",
    "#     \"general_comparison.svg\",\n",
    "#     format=\"svg\",\n",
    "#     # dpi=300,\n",
    "#     bbox_inches='tight',\n",
    "# )\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
